AI presents many risks when used in any context and careless prompting only serves to exacerbate these. One major risk is that comes from careless prompting is the return of innaccurate information. For my capstone project this risk is of the highest importance because incorrect information could have drastic effects on the team. An incorrect scouting report, if unnoticed, can lead to a faulty gameplan and decision making which disadvantages the players and could cause unfavorable results for the team. One way to mitigate this risk is to check its veracity against the game film and in game statistics. Creating a checklist of key statistics to check the accuracy of the report would allow us to catch innaccurate results before they are used in game planning. Also having a coach watch game film and compare what he saw against the generated scouting report. Both of these mitigation strategies would still be faster than creating a full scouting report without the help of artificial intelligence and also help minimize the risk associated with using artifical intelligence.

One significant risk when generating college football scouting reports with an AI model is overclaiming, where the model may assert conclusions about a team or player that are not fully supported by data. For example, the model might describe a quarterback as the “best in the conference” or a defensive unit as “impenetrable,” even if statistics suggest otherwise. This occurs because the model prioritizes producing plausible, coherent text rather than verifying facts. Overclaiming not only reduces the credibility of the scouting report but can also mislead coaches, analysts, or fans who rely on accurate assessments for decision-making. To mitigate this risk next week, a fact-checking step will be added to the workflow. Key performance metrics, such as passing yards, rushing statistics, or defensive turnovers, will be verified against official sources before the model-generated report is finalized. Additionally, the prompt will explicitly instruct the model to only present information that is supported by available data and to avoid making unverified qualitative claims. By combining structured prompts with verification, reports will be both accurate and explainable, reducing the likelihood of overstatement while maintaining clear, readable text.


One key ethical consideration in working with AI prompting is the tension between transparent reasoning and hidden reasoning. Hidden reasoning occurs when a model internally processes complex steps without exposing them to the user, which can lead to outputs that appear confident or correct but may contain errors. One risk of hidden reasoning is that users may over-rely on AI outputs without understanding the underlying assumptions or limitations, potentially resulting in misguided decisions. For example, in generating scouting reports or strategic analyses, a model might make subtle statistical misinterpretations or omit important context that goes unnoticed if the reasoning remains opaque. A practical mitigation strategy is to adopt plan-and-check or plan-first prompting, where the model explicitly outlines its reasoning steps before producing a final answer. By exposing the plan, users can review and verify each stage of reasoning, identify potential errors, and ensure the conclusions are supported by the input data. This approach increases transparency, reduces the likelihood of unnoticed mistakes, and helps maintain trust in AI-assisted decision-making. Next week, I will implement this mitigation in my capstone workflow by systematically requiring stepwise reasoning to be output alongside final results.

Formatting can greatly impact how viewers interpret content. Poor formatting can obscure strong content by making it difficult to understand, while good formatting can hide weak content because users may assume that care and accuracy went into creating it. In this project, consistent JSON formatting is required, and I check it with a validator. However, relying solely on structural validation could lead to overconfidence in the outputs and cause me to overlook incomplete, inaccurate, or low-quality content. To mitigate this risk, I will implement content checks after every test, using a binary 0 or 1 scoring system similar to what we use for formatting. This additional step ensures that outputs are not only structurally correct but also meaningful, complete, and reliable. I will specifically review key fields for null values, empty arrays, or placeholder content, and prompt the model to self-identify any missing or uncertain information. By combining structural validation with explicit content evaluation, I aim to maintain transparency and accountability in AI-assisted tasks. This approach reduces the risk of hidden weaknesses going unnoticed and helps ensure that outputs are trustworthy and useful for decision-making.

One risk when using AI to write scouting reports is plagiarism or misattribution. The model might copy parts of text or ideas from other sources without giving credit. This can happen because AI is trained on a lot of online material and sometimes repeats things it has seen before. If I don’t notice, the report could include someone else’s work or data without proper acknowledgment, which would be unethical and could hurt the credibility of my project.To reduce this risk next week, I’ll make sure the model clearly shows where its information comes from. If it uses one of my context markdown files, it will cite the file name (like 1.md or 2.md). If it doesn’t use any outside info, it will say “No context used.” I’ll also check each report myself to make sure the writing and stats are original. This helps keep the work honest and avoids plagiarism.

One key risk in using AI for scouting reports is overreach, where the model attempts to interpret or predict beyond the data provided. This can occur when the AI infers player motivation, team morale, or other qualitative judgments that are not supported by measurable evidence. Such overreach can mislead coaches or analysts into basing decisions on speculation rather than verified statistics. To mitigate this risk, I will constrain the model’s scope by explicitly instructing it to rely only on quantifiable data and cited sources. Any interpretive statements must be directly linked to observable performance trends. A second risk involves data exposure, particularly when using links or datasets that may contain private or restricted team information. Sharing or processing such data through AI tools could unintentionally violate confidentiality agreements or data privacy rules. To prevent this, all references will be limited to publicly available and officially sanctioned sources such as ESPN or Sports Reference. Additionally, I will maintain local data copies and exclude any personally identifiable or restricted content before model processing. These safeguards ensure responsible, transparent, and ethical AI use.

Balancing safety and usability is crucial to providing an effective model. Safety is obviously very important as I do not want my model to be used for any unethical behavior. Preventing attacks like injections, PII leaks, and scope creep ensures that my model is only used for its intended purpose of scouting college football teams for coaches, fans, and the media. With high profile FBI investigations currently happening in the NBA regarding private scouting information being leaked to gamblers by coaches there is even more reason to create safety protocols for my model. However safety protocols can overreach and prevent the model from being usable by its intended customers. If the model incorrectly prevents safe inputs from being used, customers will stop using the product. To mitigate the risk of unethical inputs and balance usability and safety correctly, I implemented a safety-refusal block to intercept attacks. Next week I will continue to refine this block as there are many different types of attacks (with new ones emerging all the time) and my model currently only has a 60% pass rate. I will focus on private information beign inputted into the model as that is the biggest risk in my opinion.
