AI presents many risks when used in any context and careless prompting only serves to exacerbate these. One major risk is that comes from careless prompting is the return of innaccurate information. For my capstone project this risk is of the highest importance because incorrect information could have drastic effects on the team. An incorrect scouting report, if unnoticed, can lead to a faulty gameplan and decision making which disadvantages the players and could cause unfavorable results for the team. One way to mitigate this risk is to check its veracity against the game film and in game statistics. Creating a checklist of key statistics to check the accuracy of the report would allow us to catch innaccurate results before they are used in game planning. Also having a coach watch game film and compare what he saw against the generated scouting report. Both of these mitigation strategies would still be faster than creating a full scouting report without the help of artificial intelligence and also help minimize the risk associated with using artifical intelligence.

One significant risk when generating college football scouting reports with an AI model is overclaiming, where the model may assert conclusions about a team or player that are not fully supported by data. For example, the model might describe a quarterback as the “best in the conference” or a defensive unit as “impenetrable,” even if statistics suggest otherwise. This occurs because the model prioritizes producing plausible, coherent text rather than verifying facts. Overclaiming not only reduces the credibility of the scouting report but can also mislead coaches, analysts, or fans who rely on accurate assessments for decision-making. To mitigate this risk next week, a fact-checking step will be added to the workflow. Key performance metrics, such as passing yards, rushing statistics, or defensive turnovers, will be verified against official sources before the model-generated report is finalized. Additionally, the prompt will explicitly instruct the model to only present information that is supported by available data and to avoid making unverified qualitative claims. By combining structured prompts with verification, reports will be both accurate and explainable, reducing the likelihood of overstatement while maintaining clear, readable text.


One key ethical consideration in working with AI prompting is the tension between transparent reasoning and hidden reasoning. Hidden reasoning occurs when a model internally processes complex steps without exposing them to the user, which can lead to outputs that appear confident or correct but may contain errors. One risk of hidden reasoning is that users may over-rely on AI outputs without understanding the underlying assumptions or limitations, potentially resulting in misguided decisions. For example, in generating scouting reports or strategic analyses, a model might make subtle statistical misinterpretations or omit important context that goes unnoticed if the reasoning remains opaque. A practical mitigation strategy is to adopt plan-and-check or plan-first prompting, where the model explicitly outlines its reasoning steps before producing a final answer. By exposing the plan, users can review and verify each stage of reasoning, identify potential errors, and ensure the conclusions are supported by the input data. This approach increases transparency, reduces the likelihood of unnoticed mistakes, and helps maintain trust in AI-assisted decision-making. Next week, I will implement this mitigation in my capstone workflow by systematically requiring stepwise reasoning to be output alongside final results.

Formatting can greatly impact how viewers interpret content. Poor formatting can obscure strong content by making it difficult to understand, while good formatting can hide weak content because users may assume that care and accuracy went into creating it. In this project, consistent JSON formatting is required, and I check it with a validator. However, relying solely on structural validation could lead to overconfidence in the outputs and cause me to overlook incomplete, inaccurate, or low-quality content. To mitigate this risk, I will implement content checks after every test, using a binary 0 or 1 scoring system similar to what we use for formatting. This additional step ensures that outputs are not only structurally correct but also meaningful, complete, and reliable. I will specifically review key fields for null values, empty arrays, or placeholder content, and prompt the model to self-identify any missing or uncertain information. By combining structural validation with explicit content evaluation, I aim to maintain transparency and accountability in AI-assisted tasks. This approach reduces the risk of hidden weaknesses going unnoticed and helps ensure that outputs are trustworthy and useful for decision-making.

One risk when using AI to write scouting reports is plagiarism or misattribution. The model might copy parts of text or ideas from other sources without giving credit. This can happen because AI is trained on a lot of online material and sometimes repeats things it has seen before. If I don’t notice, the report could include someone else’s work or data without proper acknowledgment, which would be unethical and could hurt the credibility of my project.To reduce this risk next week, I’ll make sure the model clearly shows where its information comes from. If it uses one of my context markdown files, it will cite the file name (like 1.md or 2.md). If it doesn’t use any outside info, it will say “No context used.” I’ll also check each report myself to make sure the writing and stats are original. This helps keep the work honest and avoids plagiarism.

One key risk in using AI for scouting reports is overreach, where the model attempts to interpret or predict beyond the data provided. This can occur when the AI infers player motivation, team morale, or other qualitative judgments that are not supported by measurable evidence. Such overreach can mislead coaches or analysts into basing decisions on speculation rather than verified statistics. To mitigate this risk, I will constrain the model’s scope by explicitly instructing it to rely only on quantifiable data and cited sources. Any interpretive statements must be directly linked to observable performance trends. A second risk involves data exposure, particularly when using links or datasets that may contain private or restricted team information. Sharing or processing such data through AI tools could unintentionally violate confidentiality agreements or data privacy rules. To prevent this, all references will be limited to publicly available and officially sanctioned sources such as ESPN or Sports Reference. Additionally, I will maintain local data copies and exclude any personally identifiable or restricted content before model processing. These safeguards ensure responsible, transparent, and ethical AI use.

One ethical risk in my evaluation process is evaluator bias, especially when relying on a single automated judge to score scouting reports. Because the judge is built from my own rubric and examples, it can unintentionally reinforce my assumptions about what “good analysis” looks like rather than objectively measuring quality. This became clear when the judge consistently misinterpreted what coaches consider actionable or meaningful, leading to unfair scoring and potentially misleading conclusions about model performance. If the evaluator systematically undervalues certain styles of strategic reasoning, it limits fairness across different types of opponents and report structures.

A realistic mitigation for next week is to integrate regular human spot checks alongside the automated rubric. Human reviews provide context-sensitive judgment the automated evaluator lacks, especially around tactical relevance and football-specific nuance. By comparing judge outputs with human assessments, I can recalibrate rubric wording or remove biased criteria, creating a more balanced and fair evaluation process overall. The prompt report recommends '" It can be useful to have a separate LLM evaluate the output and extract an answer" pp.19, but I do not think this will solve the issue of bias in grading the results.

Balancing safety and usability is crucial to providing an effective model. Safety is obviously very important as I do not want my model to be used for any unethical behavior. Preventing attacks like injections, PII leaks, and scope creep ensures that my model is only used for its intended purpose of scouting college football teams for coaches, fans, and the media. With high profile FBI investigations currently happening in the NBA regarding private scouting information being leaked to gamblers by coaches there is even more reason to create safety protocols for my model. However safety protocols can overreach and prevent the model from being usable by its intended customers. If the model incorrectly prevents safe inputs from being used, customers will stop using the product. To mitigate the risk of unethical inputs and balance usability and safety correctly, I implemented a safety-refusal block to intercept attacks. Next week I will continue to refine this block as there are many different types of attacks (with new ones emerging all the time) and my model currently only has a 60% pass rate. I will focus on private information beign inputted into the model as that is the biggest risk in my opinion.

Answering the question "Who pays the cost of cheap" is very difficult. If we are looking at that question through the lense of a business, its products, and its customers then I would say customers pay the cost of cheap but only in the short term. When businesses chose to replace previously quality parts or products with cheap alternatives they benefit almost immediately. This is because even if some of those savings are passed on to the consumer the majority almost always stays with the business. Its customers are usually left getting less value per dollar than they previously were. However I would argue that in the long term, the business has to pay it. Customers will eventually move onto other products that offer better value for the quality. Choosing to go cheap is beneficial for businesses in the long run, but can be equally detrimental in the long run. This trade-off relates to my project because although my outputs are free and thus cannot become cheaper, they can become worse in quality. If I reduce the quality of my outputs to improve some other aspect of my model I could drive my users to other products that offer better value. To mitigate this I have made quality of output my top priority in this project. Maintaining a high quality will always supercede improving latency or minimizing inputs in my capstone.

One major risk when developing a regression suite is overfitting the evaluation process itself. If I hand-pick the regression set only from teams or inputs I already know work well, I might hide real-world failures and give a false sense of system reliability. For example, my model could perform perfectly on clean, well-formatted team pages from schools like Clemson or Kentucky, yet fail completely when new teams or malformed URLs are introduced. This kind of overfitting creates the illusion of progress without improving robustness. It also makes it difficult to catch subtle data shifts, formatting anomalies, or unseen patterns that occur naturally during the season.

To mitigate this next week, I plan to refresh the regression set on a regular cadence, replacing or supplementing existing cases with newly sampled inputs from live data sources. I will also maintain a small hold-out set that remains unseen until evaluation time. These steps help reveal blind spots, encourage generalization, and ensure that testing reflects real conditions rather than a controlled comfort zone.

The Google Prompt Engineering Whitepaper says "Try not to use complex language and don’t provide unnecessary information." on page 55 and I should have taken this more to heart during this week's assignment. This is because I found that including oversized context files like full-season statistical CSVs creates both technical and ethical risks: the model can exceed token limits and fail to produce output, and repeatedly handling large raw datasets during testing increases the chance of exposing sensitive or proprietary scouting information. A practical mitigation is to preprocess these large files into smaller, team-specific summaries before passing them to the model. This reduces token usage, minimizes unnecessary data exposure, and keeps the workflow efficient and secure while still providing the model with the key information needed for accurate scouting analysis. 

In preparing my final slide deck, I focused on improving transparency so the audience clearly understands what the model can and cannot do. Slides that involve examples—such as the “Typical Output” and “Edge Case Output” sections—include explicit context explaining where the information comes from, what constraints shape the model’s behavior, and how I verify accuracy. I plan to disclose on the “Ethical Considerations” slide that all outputs are generated under a controlled prompt template with a strict JSON schema and that no player-specific or personally identifying information is ever included. Additionally, I will clarify that some outputs shown in the presentation, such as the 2002 edge case, are intentionally designed to test robustness rather than represent real scouting needs. In the “Experiments Log” slide, I will note that evaluation results were produced through structured testing in Jupyter and may not capture every possible failure mode. Overall, my goal is to make the model’s capabilities and limitations visible so the audience can assess its reliability responsibly.

