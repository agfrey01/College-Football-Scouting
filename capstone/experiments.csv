test_id,seed,temperature,top_p,top_k,max_tokens,latency_ms,cost_usd,score_0_1,notes
1,N/A,0.4,0.9,N/A,250,8920,0,0,"Very innaccurate, results cutoff due to lack of tokens"
2,N/A,0.4,0.9,N/A,500,10810,0,1,Innaccurate
3,N/A,0.6,0.9,N/A,500,10510,0,0,"Still innacurate, but wider variation. Upping the temperature definitely increased the randomness and creativity of the model"
4,N/A,0.4,0.9,N/A,500,8430,0,0,"Baseline vs. A, used few shot prompting, out was formatted perfectly, but still innaccurate"
5,N/A,0.4,0.9,N/A,500,7600,0,0,"Baseline vs. B, used chain of thought prompting, innaccurate results, but reasoning path guided results to be more acceptable"
6,N/A,0.4,0.9,N/A,500,9160,0,1,"Hard Case, using a different input format (link instead of json file), report was acceptable"
7,N/A,0.4,0.9,N/A,500,7950,0,1,"Plan-First Prompt, best test yet. The plan created was logical and executed well."
8,N/A,0.4,0.9,N/A,500,10320,0,1,"Plan-and-Check Prompt, was able to check and correct incorrect data."
9,N/A,0.4,0.9,N/A,500,8580,0,1,Chain of thought example to use a baseline to test Plan-First and Plan-and-Check
10,N/A,0.4,0.9,N/A,5000,5840,0,1,"Formatting was correct, file passed json validator from python"
11,N/A,0.4,0.9,N/A,5000,7850,0,1,"Formatting was correct, file passed json validator from python, slightly longer latency"
12,N/A,0.4,0.9,N/A,2000,8610,0,1,"Formatting was correct, file passed json validator from python, lowering tokens had minimal effect"
13,N/A,0.4,0.9,N/A,1000,7820,0,1,"Formatting was correct, file passed json validator from python, needs more tokens to effectively analyze testset"
14,N/A,0.4,0.9,N/A,1500,9200,0,1,"Formatting was correct, file passed json validator from python,  still needs more tokens, report is correct, but not useful"
15,N/A,0.4,0.9,N/A,2500,9360,0,1,"Formatting was correct, file passed json validator from python, more tokens did not resolve issue from previous test"
16,N/A,0.4,0.9,N/A,2500,8310,0,1,"Baseline test for context pack, output consistent with previous test (nothing was changed from previous test)"
17,N/A,0.4,0.9,N/A,1000000,N/A,0,0,"Error not enough tokens to complete the task. I set the max tokens to unlimited to adjust for context pack. My model maxes out at 50,000 tokens and this test surpassed that."
18,N/A,0.4,0.9,N/A,25000,17090,0,1,"Best output yet, no citations but most acceptable analysis to date"
19,N/A,0.4,0.9,N/A,25000,12040,0,1,Consults tool_calls.json for notes on test
20,N/A,0.4,0.9,N/A,25000,15430,0,1,Consults tool_calls.json for notes on test
21,N/A,0.4,0.9,N/A,25000,13980,0,1,Consults tool_calls.json for notes on test
22,N/A,0.4,0.9,N/A,25000,12020,0,1,Consults tool_calls.json for notes on test
23,N/A,0.4,0.9,N/A,25000,11470,0,1,Consults tool_calls.json for notes on test
24,N/A,0.4,0.9,N/A,25000,10980,0,1,Friction test case. No incorrect refusal
25,N/A,0.4,0.9,N/A,25000,7940,0,1,Friction test case. No incorrect refusal
26,N/A,0.4,0.9,N/A,25000,9110,0,1,Friction test case. No incorrect refusal
27,N/A,0.8,0.9,N/A,25000,15191,0,1, Parameter Sweep with adjusted temperature to 0.8. Quality and sqeed remain consistent with previous tests.
28,N/A,0.4,0.5,N/A,25000,38101,0,1, Parameter Sweep with adjusted top p to 0.5. Quality remained consistent but speed was more than double previous tests.
29,N/A,0.4,0.9,N/A,5000,15157,0,1, Parameter Sweep with adjusted max token to 5000. Speed remained consistent with previous tests. Quality regressed slightly.
30,N/A,0.8,0.9,N/A,15000,18923,0,1, Parameter Sweep with adjusted max token to 15000 and temperature to 0.8. Speed remained consistent with previous tests. Quality regressed slightly. Response was more creative and insights were better but were shorter due to lack of tokens.
31,N/A,0.6,0.9,N/A,25000,10997,0,1, Parameter Sweep with adjusted temperature to 0.6. Speed improved slightly. Quality improved slightly and was more creative.
31,N/A,0.7,0.9,N/A,25000,14343,0,1, Parameter Sweep with adjusted temperature to 0.7. Speed remained consistent with previous tests. Quality improved slightly more than last test in the same way.
32,N/A,0.7,0.9,N/A,25000,8125,0,1, A/B test on regression set. This test is with the old prompt template and context block
33,N/A,0.7,0.9,N/A,25000,N/A,0,1, A/B test on regression set. This test is with the new prompt template and context block. This prompt failed because the context file was too large. It overran my API limit on token usage.



